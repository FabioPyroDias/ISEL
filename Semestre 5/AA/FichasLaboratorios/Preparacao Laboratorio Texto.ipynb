{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a38302d",
   "metadata": {},
   "source": [
    "## Base de Dados IMDb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d02ce78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77daee43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000\n"
     ]
    }
   ],
   "source": [
    "D = pickle.load(open(\"PreparacaoLaboratorioTexto/imdbCriticas.p\", 'rb'))\n",
    "Docs = D.data\n",
    "y = D.target\n",
    "\n",
    "print(len(Docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eedbc787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88214\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer().fit(Docs)\n",
    "tokens = tfidf.get_feature_names()\n",
    "\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bbd8675",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Docs = [doc.decode('UTF-8') for doc in Docs] #O engenheiro faz este passo para convertir a palavra em string.\n",
    "                                              #No nosso caso, a palavra já vem em string.  \n",
    "Docs = [doc.replace('<br />', ' ') for doc in Docs]\n",
    "Docs = [re.sub(r'[^a-zA-Z]+', ' ', doc) for doc in Docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691f95c4",
   "metadata": {},
   "source": [
    "Especificar minimo de ocorrências nos documentos\n",
    "\n",
    "<b>Isto só deve ser feito após um pré-tratamento dos dados</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc32c8aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35140\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(min_df=4).fit(Docs)\n",
    "tokens = tfidf.get_feature_names()\n",
    "\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015e0494",
   "metadata": {},
   "source": [
    "Para além do número minimo de ocorrências, também podemos especificar um padrão."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "377cfd07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33657\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(min_df=4, token_pattern=r'\\b\\w\\w\\w\\w+\\b').fit(Docs)\n",
    "tokens = tfidf.get_feature_names()\n",
    "\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8bee8d",
   "metadata": {},
   "source": [
    "## Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d191f0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b7f3855",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "\"you better start swimming or you’ll sink like a stone for the times they are a-changing\",\n",
    "\"the loser now will be later to win cause the times they are a-changing\",\n",
    "\"it’ll soon shake your windows and rattle your walls for the times they are a-changing\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "985d6b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer().fit(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6932eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n"
     ]
    }
   ],
   "source": [
    "voc = cv.vocabulary_\n",
    "\n",
    "print(len(voc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e07431b",
   "metadata": {},
   "source": [
    "Isto indica-nos que o nosso corpus possui 31 palavras diferentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "736b3f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'you': 29, 'better': 3, 'start': 18, 'swimming': 20, 'or': 13, 'll': 10, 'sink': 16, 'like': 9, 'stone': 19, 'for': 6, 'the': 21, 'times': 23, 'they': 22, 'are': 1, 'changing': 5, 'loser': 11, 'now': 12, 'will': 26, 'be': 2, 'later': 8, 'to': 24, 'win': 27, 'cause': 4, 'it': 7, 'soon': 17, 'shake': 15, 'your': 30, 'windows': 28, 'and': 0, 'rattle': 14, 'walls': 25}\n"
     ]
    }
   ],
   "source": [
    "print(voc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5faaf4",
   "metadata": {},
   "source": [
    "Ao fazer o seu print, este não vem ordenado. <b>Não esquecer que o voc é um dicionário</b>\n",
    "\n",
    "Para ordenar o dicionário:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70e3b1f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'and': 0, 'are': 1, 'be': 2, 'better': 3, 'cause': 4, 'changing': 5, 'for': 6, 'it': 7, 'later': 8, 'like': 9, 'll': 10, 'loser': 11, 'now': 12, 'or': 13, 'rattle': 14, 'shake': 15, 'sink': 16, 'soon': 17, 'start': 18, 'stone': 19, 'swimming': 20, 'the': 21, 'they': 22, 'times': 23, 'to': 24, 'walls': 25, 'will': 26, 'win': 27, 'windows': 28, 'you': 29, 'your': 30}\n"
     ]
    }
   ],
   "source": [
    "voc = {k: v for k, v in sorted(voc.items(), key = lambda item: item[1])}\n",
    "\n",
    "print(voc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5458a1a8",
   "metadata": {},
   "source": [
    "Para dar print e aparecer de forma ordenada, sem necessitar de organizar o voc, usamos o cv.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bfe890be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'are', 'be', 'better', 'cause', 'changing', 'for', 'it', 'later', 'like', 'll', 'loser', 'now', 'or', 'rattle', 'shake', 'sink', 'soon', 'start', 'stone', 'swimming', 'the', 'they', 'times', 'to', 'walls', 'will', 'win', 'windows', 'you', 'your']\n"
     ]
    }
   ],
   "source": [
    "print(cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c867293c",
   "metadata": {},
   "source": [
    "Até agora, a chave do dicionário tem sido a palavra. Se quisermos inverter isto, para que a chave seja o número e o valor a palavra, podemos usar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e26b812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'and', 1: 'are', 2: 'be', 3: 'better', 4: 'cause', 5: 'changing', 6: 'for', 7: 'it', 8: 'later', 9: 'like', 10: 'll', 11: 'loser', 12: 'now', 13: 'or', 14: 'rattle', 15: 'shake', 16: 'sink', 17: 'soon', 18: 'start', 19: 'stone', 20: 'swimming', 21: 'the', 22: 'they', 23: 'times', 24: 'to', 25: 'walls', 26: 'will', 27: 'win', 28: 'windows', 29: 'you', 30: 'your'}\n"
     ]
    }
   ],
   "source": [
    "voc2 = {value: key for key, value in voc.items()}\n",
    "\n",
    "print(voc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a4f3cd",
   "metadata": {},
   "source": [
    "##### Vamos agora transformar o nosso corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "41ef3101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method spmatrix.getformat of <3x31 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 43 stored elements in Compressed Sparse Row format>>\n"
     ]
    }
   ],
   "source": [
    "X = cv.transform(corpus)\n",
    "\n",
    "print(X.getformat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13f09e2",
   "metadata": {},
   "source": [
    "Isto indicanos que possuimos uma matrix \"sparse\" de 3 por 31.\n",
    "- 3: Número de documentos. Neste caso, frases diferentes.\n",
    "- 31: Palavras encontradas nesses documentos.\n",
    "\n",
    "Se dermos print ao corpus transformado, X, esta vai-nos apresentar quais as dimensões que estão diferentes a 0.<br>\n",
    "Ou seja, quais as palavras no corpus que se encontram nessa frase (documento)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0256f63c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 5)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 9)\t1\n",
      "  (0, 10)\t1\n",
      "  (0, 13)\t1\n",
      "  (0, 16)\t1\n",
      "  (0, 18)\t1\n",
      "  (0, 19)\t1\n",
      "  (0, 20)\t1\n",
      "  (0, 21)\t1\n",
      "  (0, 22)\t1\n",
      "  (0, 23)\t1\n",
      "  (0, 29)\t2\n",
      "  (1, 1)\t1\n",
      "  (1, 2)\t1\n",
      "  (1, 4)\t1\n",
      "  (1, 5)\t1\n",
      "  (1, 8)\t1\n",
      "  (1, 11)\t1\n",
      "  (1, 12)\t1\n",
      "  (1, 21)\t2\n",
      "  (1, 22)\t1\n",
      "  (1, 23)\t1\n",
      "  (1, 24)\t1\n",
      "  (1, 26)\t1\n",
      "  (1, 27)\t1\n",
      "  (2, 0)\t1\n",
      "  (2, 1)\t1\n",
      "  (2, 5)\t1\n",
      "  (2, 6)\t1\n",
      "  (2, 7)\t1\n",
      "  (2, 10)\t1\n",
      "  (2, 14)\t1\n",
      "  (2, 15)\t1\n",
      "  (2, 17)\t1\n",
      "  (2, 21)\t1\n",
      "  (2, 22)\t1\n",
      "  (2, 23)\t1\n",
      "  (2, 25)\t1\n",
      "  (2, 28)\t1\n",
      "  (2, 30)\t2\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f7bcc8",
   "metadata": {},
   "source": [
    "Para nos ser mais fácil a visualização destas ocorrências, podemos imprimi-la como formato de array.\n",
    "\n",
    "<b>Se existirem muitos documentos ou muitas dimensões, isto torna-se extremamente ineficiente devido à mudança de uma matriz \"sparse\" para um array</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7546666c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 1 0 1 1 0 0 1 1 0 0 1 0 0 1 0 1 1 1 1 1 1 0 0 0 0 0 2 0]\n",
      " [0 1 1 0 1 1 0 0 1 0 0 1 1 0 0 0 0 0 0 0 0 2 1 1 1 0 1 1 0 0 0]\n",
      " [1 1 0 0 0 1 1 1 0 0 1 0 0 0 1 1 0 1 0 0 0 1 1 1 0 1 0 0 1 0 2]]\n"
     ]
    }
   ],
   "source": [
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb3fb30",
   "metadata": {},
   "source": [
    "Como foi previamente concluído, existem 3 frases (documentos) e 31 palavras.<br>\n",
    "Isto traduz-se num array 3x31.\n",
    "\n",
    "###### Analisando a primeira linha:\n",
    "\n",
    "Como podemos observar, existem 15 dimensões diferentes de 0 e a última destas é 2.<br>\n",
    "Se formos observar qual a palavra que está associada à segunda dimensão, index 1, é \"are\". E de facto, esta encontra-se na frase:\n",
    "\n",
    "- you better start swimming or you’ll sink like a stone for the times they <b>are</b> a-changing\n",
    "\n",
    "A existência do 2 significa que uma palavra do nosso dicionário, aparece duas vezes na frase.<br>\n",
    "Se identificarmos qual é a palavra correspondente (index 29) concluímos que é um \"you\".<br>\n",
    "Analisando novamente a frase:\n",
    "- <b>you</b> better start swimming or <b>you</b>’ll sink like a stone for the times they are a-changing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57d35a8",
   "metadata": {},
   "source": [
    "###### Vamos agora obter o número de ocorrências de cada palavra nos documentos todos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0eeec536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 3 1 1 1 3 2 1 1 1 2 1 1 1 1 1 1 1 1 1 1 4 3 3 1 1 1 1 1 2 2]\n"
     ]
    }
   ],
   "source": [
    "count = np.sum(X.toarray(), axis = 0)\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62585c1e",
   "metadata": {},
   "source": [
    "Podemos verificar que algumas ocorrem mais que uma vez. Neste caso, o maior número de ocorrências das palavra é 4.\n",
    "\n",
    "Vamos imprimir isto de uma forma legível, com a devida identificação do index, palavra e do número de ocorrências:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "80511191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index:  0  Palavra:  and  - Ocorrências:  1\n",
      "Index:  1  Palavra:  are  - Ocorrências:  3\n",
      "Index:  2  Palavra:  be  - Ocorrências:  1\n",
      "Index:  3  Palavra:  better  - Ocorrências:  1\n",
      "Index:  4  Palavra:  cause  - Ocorrências:  1\n",
      "Index:  5  Palavra:  changing  - Ocorrências:  3\n",
      "Index:  6  Palavra:  for  - Ocorrências:  2\n",
      "Index:  7  Palavra:  it  - Ocorrências:  1\n",
      "Index:  8  Palavra:  later  - Ocorrências:  1\n",
      "Index:  9  Palavra:  like  - Ocorrências:  1\n",
      "Index:  10  Palavra:  ll  - Ocorrências:  2\n",
      "Index:  11  Palavra:  loser  - Ocorrências:  1\n",
      "Index:  12  Palavra:  now  - Ocorrências:  1\n",
      "Index:  13  Palavra:  or  - Ocorrências:  1\n",
      "Index:  14  Palavra:  rattle  - Ocorrências:  1\n",
      "Index:  15  Palavra:  shake  - Ocorrências:  1\n",
      "Index:  16  Palavra:  sink  - Ocorrências:  1\n",
      "Index:  17  Palavra:  soon  - Ocorrências:  1\n",
      "Index:  18  Palavra:  start  - Ocorrências:  1\n",
      "Index:  19  Palavra:  stone  - Ocorrências:  1\n",
      "Index:  20  Palavra:  swimming  - Ocorrências:  1\n",
      "Index:  21  Palavra:  the  - Ocorrências:  4\n",
      "Index:  22  Palavra:  they  - Ocorrências:  3\n",
      "Index:  23  Palavra:  times  - Ocorrências:  3\n",
      "Index:  24  Palavra:  to  - Ocorrências:  1\n",
      "Index:  25  Palavra:  walls  - Ocorrências:  1\n",
      "Index:  26  Palavra:  will  - Ocorrências:  1\n",
      "Index:  27  Palavra:  win  - Ocorrências:  1\n",
      "Index:  28  Palavra:  windows  - Ocorrências:  1\n",
      "Index:  29  Palavra:  you  - Ocorrências:  2\n",
      "Index:  30  Palavra:  your  - Ocorrências:  2\n"
     ]
    }
   ],
   "source": [
    "for i in range(31):\n",
    "    print(\"Index: \", i, \" Palavra: \", voc2[i], \" - Ocorrências: \", count[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17741714",
   "metadata": {},
   "source": [
    "Assim, podemos concluir que a palavra que ocorre mais vezes é a palavra \"the\", com quatro ocorrências e o seu index no vocabulário ordenado é o 21."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f380c025",
   "metadata": {},
   "source": [
    "## Representação tf-idf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e805105d",
   "metadata": {},
   "source": [
    "Este modelo é não-supervisionado. Portanto, este não tem atenção se a critica é positiva ou negativa, ou qual a sua pontuação.<br>\n",
    "O que este modelo faz é considerar as palavras importantes:\n",
    "- Palavras que aparecem muitas vezes num dado documento, mas em poucos documentos no geral. Em princípio, estas palavras serão boas caracterizadoras desse documento.\n",
    "- Palavras que aparecem na maioria dos documentos, não caracterizam bem os documentos e o seu valor td-idf será baixo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7675433c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "250c8db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "\"you better start swimming or you’ll sink like a stone for the times they are a-changing\",\n",
    "\"the loser now will be later to win cause the times they are a-changing\",\n",
    "\"it’ll soon shake your windows and rattle your walls for the times they are a-changing\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e113025a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer().fit(corpus)\n",
    "X = cv.transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76058627",
   "metadata": {},
   "source": [
    "Em vez do número de ocorrências, podemos normalizar os valores ao longo dos documentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d538a06a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 1 0 1 1 0 0 1 1 0 0 1 0 0 1 0 1 1 1 1 1 1 0 0 0 0 0 2 0]\n",
      " [0 1 1 0 1 1 0 0 1 0 0 1 1 0 0 0 0 0 0 0 0 2 1 1 1 0 1 1 0 0 0]\n",
      " [1 1 0 0 0 1 1 1 0 0 1 0 0 0 1 1 0 1 0 0 0 1 1 1 0 1 0 0 1 0 2]]\n",
      "[[0.   0.16 0.   0.27 0.   0.16 0.2  0.   0.   0.27 0.2  0.   0.   0.27\n",
      "  0.   0.   0.27 0.   0.27 0.27 0.27 0.16 0.16 0.16 0.   0.   0.   0.\n",
      "  0.   0.54 0.  ]\n",
      " [0.   0.18 0.3  0.   0.3  0.18 0.   0.   0.3  0.   0.   0.3  0.3  0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.36 0.18 0.18 0.3  0.   0.3  0.3\n",
      "  0.   0.   0.  ]\n",
      " [0.27 0.16 0.   0.   0.   0.16 0.2  0.27 0.   0.   0.2  0.   0.   0.\n",
      "  0.27 0.27 0.   0.27 0.   0.   0.   0.16 0.16 0.16 0.   0.27 0.   0.\n",
      "  0.27 0.   0.54]]\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfTransformer().fit(X)\n",
    "X2 = tfidf.transform(X).toarray()\n",
    "\n",
    "print(X.toarray())\n",
    "print(np.round(X2, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b4db67",
   "metadata": {},
   "source": [
    "Se somarmos todos os valores das colunas, ou seja, as ocorrências das palavras para cada frase (documento), podemos observar que o seu valor será 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a4b03a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(X2**2, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a7b61e",
   "metadata": {},
   "source": [
    "Em vez de usar o CountVectorizer em conjunção com o TfidfTransformer, podemos usar o TfidfVectorizer directamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e3272080",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer().fit(corpus)\n",
    "X3 = tfidf.transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b0a458",
   "metadata": {},
   "source": [
    "Se compararmos os resultados obtidos anteriormente na conjunção do CountVectorizer e TfidfTransformer com os actuais, obtidos pelo TfidfVectorizer, concluímos que são iguais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "40f71026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.15841025 0.         0.26821186 0.        ]\n",
      " [0.         0.17979686 0.30442255 0.         0.30442255]\n",
      " [0.26821186 0.15841025 0.         0.         0.        ]]\n",
      "\n",
      "[[0.         0.15841025 0.         0.26821186 0.        ]\n",
      " [0.         0.17979686 0.30442255 0.         0.30442255]\n",
      " [0.26821186 0.15841025 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(X2[:, :5])\n",
    "print()\n",
    "print(X3[:, :5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199e8e2b",
   "metadata": {},
   "source": [
    "## Base de Dados IMDb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "caa92217",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f41d2d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])\n"
     ]
    }
   ],
   "source": [
    "D = load_files('PreparacaoLaboratorioTexto/aclImdb/train/')\n",
    "\n",
    "print(D.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "27478a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>   25000\n",
      "<class 'numpy.ndarray'>   25000\n",
      "<class 'numpy.ndarray'>   25000\n"
     ]
    }
   ],
   "source": [
    "X = D.data\n",
    "y = D.target\n",
    "fn = D.filenames\n",
    "\n",
    "print(type(X), \" \", len(X))\n",
    "print(type(y), \" \", len(y))\n",
    "print(type(fn), \" \", len(fn))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cafd0f",
   "metadata": {},
   "source": [
    "- Neste caso, a variável X possui os comentários (o que chamámos de documentos).\n",
    "- A variável y possui o rating da critica\n",
    "- A variável fn possui os nomes dos documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aae1199d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\"Zero Day leads you to think, even re-think why two boys/young men would do what they did - commit mutual suicide via slaughtering their classmates. It captures what must be beyond a bizarre mode of being for two humans who have decided to withdraw from common civility in order to define their own/mutual world via coupled destruction.<br /><br />It is not a perfect movie but given what money/time the filmmaker and actors had - it is a remarkable product. In terms of explaining the motives and actions of the two young suicide/murderers it is better than 'Elephant' - in terms of being a film that gets under our 'rationalistic' skin it is a far, far better film than almost anything you are likely to see. <br /><br />Flawed but honest with a terrible honesty.\"  -  1  :  PreparacaoLaboratorioTexto/aclImdb/train/pos\\11485_10.txt\n"
     ]
    }
   ],
   "source": [
    "print(X[0], \" - \", y[0], \" : \", fn[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c3a7da",
   "metadata": {},
   "source": [
    "Também podemos identificar o tipo de classes que o y possui.\n",
    "\n",
    "Isto provém das sub-pastas de onde estes dados foram carregados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "74c56d05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neg', 'pos']\n"
     ]
    }
   ],
   "source": [
    "print(D.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e722de7d",
   "metadata": {},
   "source": [
    "### Limpeza do Texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6e09133a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3fff65f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = load_files('PreparacaoLaboratorioTexto/aclImdb/train/')\n",
    "Docs = D.data\n",
    "y = D.target\n",
    "fNames = D.filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d2ea0800",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer().fit(Docs)\n",
    "tokens = tfidf.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4d3834d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74849\n",
      "['00', '000', '0000000000001', '00001', '00015', '000s', '001', '003830', '006', '007', '0079', '0080', '0083', '0093638', '00am', '00pm', '00s', '01', '01pm', '02', '020410', '029', '03', '04', '041', '05', '050', '06', '06th', '07', '08', '087', '089', '08th', '09', '0f', '0ne', '0r', '0s', '10', '100', '1000', '1000000', '10000000000000', '1000lb', '1000s', '1001', '100b', '100k', '100m']\n"
     ]
    }
   ],
   "source": [
    "print(len(tokens))\n",
    "print(tokens[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "790f1195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\"Zero Day leads you to think, even re-think why two boys/young men would do what they did - commit mutual suicide via slaughtering their classmates. It captures what must be beyond a bizarre mode of being for two humans who have decided to withdraw from common civility in order to define their own/mutual world via coupled destruction.<br /><br />It is not a perfect movie but given what money/time the filmmaker and actors had - it is a remarkable product. In terms of explaining the motives and actions of the two young suicide/murderers it is better than 'Elephant' - in terms of being a film that gets under our 'rationalistic' skin it is a far, far better film than almost anything you are likely to see. <br /><br />Flawed but honest with a terrible honesty.\"\n"
     ]
    }
   ],
   "source": [
    "print(Docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f7a7d3",
   "metadata": {},
   "source": [
    "Como podemos observar, se apenas fizermos isto, ficamos com cerca de 75000 dimensões. Isto vai-se revelar extremamente problemático.<br>\n",
    "\n",
    "Vamos então tratar de limpar os dados para os puder analisar depois.\n",
    "- O texto vem em string binária, logo temos de <b>converter para strings válidas</b>.\n",
    "- Também existem caracteres de mudança de linha em HTML que têm de ser <b>removidos</b>.\n",
    "- Existem caracteres que não são \"palavras\" consideradas válidas. Devemos <b>remover o que não pertence à expressão passada</b>.\n",
    "    - 'a-zA-Z' inclui todos os caracteres \"normais\". A seguinte parte da expressão significa que inclui os caracteres 192 até ao 255. Estes são os acentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "597b9650",
   "metadata": {},
   "outputs": [],
   "source": [
    "Docs = [doc.decode('UTF-8') for doc in Docs]\n",
    "Docs = [doc.replace('<br />', ' ') for doc in Docs]\n",
    "Docs = [re.sub(r'[^a-zA-Z\\u00C0-\\u00FF]+', ' ', doc) for doc in Docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2a83af",
   "metadata": {},
   "source": [
    "Agora podemos inspeccionar o resultado destas operações."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "66e75d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero Day leads you to think even re think why two boys young men would do what they did commit mutual suicide via slaughtering their classmates It captures what must be beyond a bizarre mode of being for two humans who have decided to withdraw from common civility in order to define their own mutual world via coupled destruction It is not a perfect movie but given what money time the filmmaker and actors had it is a remarkable product In terms of explaining the motives and actions of the two young suicide murderers it is better than Elephant in terms of being a film that gets under our rationalistic skin it is a far far better film than almost anything you are likely to see Flawed but honest with a terrible honesty \n"
     ]
    }
   ],
   "source": [
    "print(Docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2e0314",
   "metadata": {},
   "source": [
    "Com estas operações, o número de dimensões foi, mas muito pouco reduzido. Assim, finalizamos a limpeza do texto.\n",
    "\n",
    "Ao usar o tfidfVectorizer, podemos passar alguns parametros que vão diminuir imenso o número de dimensões (palavras) a ter em conta na análise dos documentos.\n",
    "- min_df: Este parametro específica o número mínimo de ocorrências necessárias, nos documentos todos, para essa palavra ser incluída como dimensão.\n",
    "- token-pattern: Este recebe uma string em formato padrão. \\b significa backspace, \\w significa caracter e + significa que o \"símbolo\" anterior pode aparecer mais vezes.\n",
    "    - Abaixo, estamos a especificar que o só serão incluídas palavras que tenham um espaço antes e depois dos caracteres, e tem de ter no mínimo 3 caracteres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6b9882f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(min_df = 3, token_pattern = r'\\b\\w\\w\\w+\\b').fit(Docs)\n",
    "tokens = tfidf.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1d31d1aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35122\n",
      "['aaa', 'aaargh', 'aag', 'aage', 'aames', 'aamir', 'aankhen', 'aapke', 'aardman', 'aargh', 'aaron', 'aatish', 'aback', 'abandon', 'abandoned', 'abandoning', 'abandonment', 'abandons', 'abbas', 'abbey', 'abbot', 'abbott', 'abbreviated', 'abby', 'abc', 'abduct', 'abducted', 'abducting', 'abduction', 'abductor', 'abdul', 'abe', 'abel', 'abercrombie', 'aberration', 'abetted', 'abhay', 'abhishek', 'abhorrence', 'abhorrent', 'abide', 'abiding', 'abigail', 'abilities', 'ability', 'abject', 'ablaze', 'able', 'ably', 'abnormal']\n"
     ]
    }
   ],
   "source": [
    "print(len(tokens))\n",
    "print(tokens[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8678c7c",
   "metadata": {},
   "source": [
    "### Stemmers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "119327e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f4c6ca54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35122\n",
      "['aaa', 'aaargh', 'aag', 'aage', 'aames', 'aamir', 'aankhen', 'aapke', 'aardman', 'aargh', 'aaron', 'aatish', 'aback', 'abandon', 'abandoned', 'abandoning', 'abandonment', 'abandons', 'abbas', 'abbey', 'abbot', 'abbott', 'abbreviated', 'abby', 'abc', 'abduct', 'abducted', 'abducting', 'abduction', 'abductor', 'abdul', 'abe', 'abel', 'abercrombie', 'aberration', 'abetted', 'abhay', 'abhishek', 'abhorrence', 'abhorrent', 'abide', 'abiding', 'abigail', 'abilities', 'ability', 'abject', 'ablaze', 'able', 'ably', 'abnormal']\n"
     ]
    }
   ],
   "source": [
    "stemFunc = PorterStemmer()\n",
    "voc = tfidf.get_feature_names()\n",
    "print(len(voc))\n",
    "print(voc[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f2cd1d",
   "metadata": {},
   "source": [
    "Neste vocabulário, podemos identificar várias palavras muito parecidas que possuem significados muito semelhantes.<br>\n",
    "\"abandoned\", \"abandoning\", \"abandonment\", \"abandons\".\n",
    "\n",
    "Ao usar o Stemmer, podemos converter todas estas palavras para um única palavra \"raiz\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7e2c49b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22470\n",
      "['aaa', 'aaargh', 'aag', 'aag', 'aam', 'aamir', 'aankhen', 'aapk', 'aardman', 'aargh', 'aaron', 'aatish', 'aback', 'abandon', 'abandon', 'abandon', 'abandon', 'abandon', 'abba', 'abbey', 'abbot', 'abbott', 'abbrevi', 'abbi', 'abc', 'abduct', 'abduct', 'abduct', 'abduct', 'abductor', 'abdul', 'abe', 'abel', 'abercrombi', 'aberr', 'abet', 'abhay', 'abhishek', 'abhorr', 'abhorr', 'abid', 'abid', 'abigail', 'abil', 'abil', 'abject', 'ablaz', 'abl', 'abli', 'abnorm']\n"
     ]
    }
   ],
   "source": [
    "voc2 = [stemFunc.stem(w) for w in voc]\n",
    "\n",
    "print(len(np.unique(voc2)))\n",
    "print(voc2[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891c2c73",
   "metadata": {},
   "source": [
    "Observamos que as palavras previamente distintas são substituídas por \"abandon\". Esta é a palavra \"raiz\" de todas elas.<br>\n",
    "Também podemos ver a redução de 12000 dimensões.\n",
    "\n",
    "Mas, mesmo fazendo isto, a dimensão no vocabulário manteve-se. Abordaremos isto mais à frente.<br>\n",
    "Vamos fazer o stem para cada documento.\n",
    "\n",
    "Começamos por usar o método \"split\" das strings que converte todas as palavras nessa string numa lista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7c0df566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Zero', 'Day', 'leads', 'you', 'to', 'think', 'even', 're', 'think', 'why', 'two', 'boys', 'young', 'men', 'would', 'do', 'what', 'they', 'did', 'commit', 'mutual', 'suicide', 'via', 'slaughtering', 'their', 'classmates', 'It', 'captures', 'what', 'must', 'be', 'beyond', 'a', 'bizarre', 'mode', 'of', 'being', 'for', 'two', 'humans', 'who', 'have', 'decided', 'to', 'withdraw', 'from', 'common', 'civility', 'in', 'order', 'to', 'define', 'their', 'own', 'mutual', 'world', 'via', 'coupled', 'destruction', 'It', 'is', 'not', 'a', 'perfect', 'movie', 'but', 'given', 'what', 'money', 'time', 'the', 'filmmaker', 'and', 'actors', 'had', 'it', 'is', 'a', 'remarkable', 'product', 'In', 'terms', 'of', 'explaining', 'the', 'motives', 'and', 'actions', 'of', 'the', 'two', 'young', 'suicide', 'murderers', 'it', 'is', 'better', 'than', 'Elephant', 'in', 'terms', 'of', 'being', 'a', 'film', 'that', 'gets', 'under', 'our', 'rationalistic', 'skin', 'it', 'is', 'a', 'far', 'far', 'better', 'film', 'than', 'almost', 'anything', 'you', 'are', 'likely', 'to', 'see', 'Flawed', 'but', 'honest', 'with', 'a', 'terrible', 'honesty']\n"
     ]
    }
   ],
   "source": [
    "doc = Docs[0]\n",
    "dd = doc.split()\n",
    "\n",
    "print(dd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a76905",
   "metadata": {},
   "source": [
    "O método contrário a este é o \"join\". Este converte um array numa string.\n",
    "\n",
    "Se meteremos um espaço, ' ', seguido do .join definimos que o que se vai encontrar entre cada termo é um espaço."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e334653e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Zero Day leads you to think even re think why two boys young men would do what they did commit mutual suicide via slaughtering their classmates It captures what must be beyond a bizarre mode of being for two humans who have decided to withdraw from common civility in order to define their own mutual world via coupled destruction It is not a perfect movie but given what money time the filmmaker and actors had it is a remarkable product In terms of explaining the motives and actions of the two young suicide murderers it is better than Elephant in terms of being a film that gets under our rationalistic skin it is a far far better film than almost anything you are likely to see Flawed but honest with a terrible honesty'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(dd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8dbb907",
   "metadata": {},
   "source": [
    "Apresentado isto, a metodologia para o uso do stem para os documentos é:\n",
    "- Separar a string para uma lista\n",
    "- Passar essa lista pelo stemmer\n",
    "- Unir a lista para uma string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a807dd62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: \n",
      "Zero Day leads you to think even re think why two boys young men would do what they did commit mutual suicide via slaughtering their classmates It captures what must be beyond a bizarre mode of being for two humans who have decided to withdraw from common civility in order to define their own mutual world via coupled destruction It is not a perfect movie but given what money time the filmmaker and actors had it is a remarkable product In terms of explaining the motives and actions of the two young suicide murderers it is better than Elephant in terms of being a film that gets under our rationalistic skin it is a far far better film than almost anything you are likely to see Flawed but honest with a terrible honesty \n",
      "\n",
      "//-----------------//-----------------//-----------------//\n",
      "\n",
      "Após Stem: \n",
      "zero day lead you to think even re think whi two boy young men would do what they did commit mutual suicid via slaughter their classmat it captur what must be beyond a bizarr mode of be for two human who have decid to withdraw from common civil in order to defin their own mutual world via coupl destruct it is not a perfect movi but given what money time the filmmak and actor had it is a remark product in term of explain the motiv and action of the two young suicid murder it is better than eleph in term of be a film that get under our rationalist skin it is a far far better film than almost anyth you are like to see flaw but honest with a terribl honesti\n"
     ]
    }
   ],
   "source": [
    "doc2 = ' '.join([stemFunc.stem(w) for w in doc.split()])\n",
    "\n",
    "print(\"Original: \")\n",
    "print(doc)\n",
    "print()\n",
    "print(\"//-----------------//-----------------//-----------------//\")\n",
    "print()\n",
    "print(\"Após Stem: \")\n",
    "print(doc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b230cedb",
   "metadata": {},
   "source": [
    "Comparando os textos percebemos que certas palavras são simplificadas para as suas raízes. Pegando na primeira linha:\n",
    "\n",
    "\"zero day <b>lead</b> you to think even re think <b>whi</b> two boy young men would do what they did commit mutual <b>suicid</b> via <b>slaughter</b> their\"\n",
    "\n",
    "Esta é a ideia geral do que vai acontecer a cada documento ao passar pelo Stemmer. <br>\n",
    "Vamos fazer isto para todos os documentos e passá-lo, seguidamente, pelo TfidfVectorizer com os parametros vistos anteriormente.\n",
    "\n",
    "No próximo exemplo, vamos fazer tudo de novo, realmente representando o processo completo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2035fd8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23220\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "D = load_files('PreparacaoLaboratorioTexto/aclImdb/train/')\n",
    "Docs = D.data\n",
    "y = D.target\n",
    "fNames = D.filenames\n",
    "\n",
    "Docs = [doc.decode('UTF-8') for doc in Docs]\n",
    "Docs = [doc.replace('<br />', ' ') for doc in Docs]\n",
    "Docs = [re.sub(r'[^a-zA-Z\\u00C0-\\u00FF]+', ' ', doc) for doc in Docs]\n",
    "\n",
    "stemFunc = PorterStemmer()\n",
    "Docs2 =[' '.join([stemFunc.stem(w) for w in d.split()]) for d in Docs]\n",
    "\n",
    "tfidf = TfidfVectorizer(min_df = 3, token_pattern = r'\\b\\w\\w\\w+\\b').fit(Docs2)\n",
    "tokens = tfidf.get_feature_names()\n",
    "\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024ad461",
   "metadata": {},
   "source": [
    "Inicialmente, os nossos dados possuiam 74849 e, após a limpeza do texto e a sua representação TfidfVectorizer, obtemos 23220.<br>\n",
    "Isto significa que removemos 51629 palavras (dimensões).\n",
    "\n",
    "Vamos agora observar para os outros dois tipos de Stemmer: SnowballStemmer e LancasterStemmer.\n",
    "\n",
    "<b>No vídeo parte 6, 23:32 o engenheiro diz que o modelo tf-idf não é usado nos ficheiros de teste. Questionar!</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f3234e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22874\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "stemFunc = SnowballStemmer(language='english')\n",
    "Docs2 =[' '.join([stemFunc.stem(w) for w in d.split()]) for d in Docs]\n",
    "\n",
    "tfidf2 = TfidfVectorizer(min_df = 3, token_pattern = r'\\b\\w\\w\\w+\\b').fit(Docs2)\n",
    "tokens = tfidf2.get_feature_names()\n",
    "\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b3215a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18959\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "stemFunc = LancasterStemmer()\n",
    "Docs2 =[' '.join([stemFunc.stem(w) for w in d.split()]) for d in Docs]\n",
    "\n",
    "tfidf3 = TfidfVectorizer(min_df = 3, token_pattern = r'\\b\\w\\w\\w+\\b').fit(Docs2)\n",
    "tokens = tfidf3.get_feature_names()\n",
    "\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cc23d2",
   "metadata": {},
   "source": [
    "Podemos observar que houve cerca de 500 palavras removidas do PorterStemmer para o SnowballStemmer. Mas a grande diferença está no Lancaster Stemmer, com um redução de 4000 palavras. Isto porque o LancasterStemmer é mais estrito.\n",
    "Vamos observar as primeiras 50 amostras de cada um dos Stemmers e compará-los"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7c5f524e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PorterStemmer:\n",
      "['aaa', 'aaargh', 'aag', 'aam', 'aamir', 'aankhen', 'aapk', 'aardman', 'aargh', 'aaron', 'aatish', 'aback', 'abandon', 'abba', 'abbey', 'abbi', 'abbot', 'abbott', 'abbrevi', 'abc', 'abdic', 'abduct', 'abductor', 'abdul', 'abe', 'abel', 'abercrombi', 'aberr', 'abet', 'abhay', 'abhishek', 'abhor', 'abhorr', 'abid', 'abigail', 'abil', 'abject', 'abl', 'ablaz', 'abli', 'abnorm', 'abo', 'aboard', 'abod', 'abolish', 'abolit', 'abomin', 'aborigin', 'abort', 'abortionist']\n",
      "\n",
      "//-----------------//-----------------//-----------------//\n",
      "\n",
      "SnowballStemmer:\n",
      "['aaa', 'aaargh', 'aag', 'aam', 'aamir', 'aankhen', 'aapk', 'aardman', 'aargh', 'aaron', 'aatish', 'aback', 'abandon', 'abba', 'abbey', 'abbi', 'abbot', 'abbott', 'abbrevi', 'abc', 'abdic', 'abduct', 'abductor', 'abdul', 'abe', 'abel', 'abercrombi', 'aberr', 'abet', 'abhay', 'abhishek', 'abhor', 'abhorr', 'abid', 'abigail', 'abil', 'abject', 'abl', 'ablaz', 'abli', 'abnorm', 'abo', 'aboard', 'abod', 'abolish', 'abolit', 'abomin', 'aborigin', 'abort', 'abortionist']\n",
      "\n",
      "//-----------------//-----------------//-----------------//\n",
      "\n",
      "LancasterStemmer:\n",
      "['aaargh', 'aag', 'aam', 'aamir', 'aankh', 'aapk', 'aardm', 'aargh', 'aaron', 'aat', 'aback', 'abandon', 'abba', 'abbey', 'abbot', 'abbrevy', 'abby', 'abc', 'abd', 'abduc', 'abel', 'aber', 'abercromby', 'abet', 'abh', 'abhay', 'abhishek', 'abhor', 'abid', 'abigail', 'abject', 'abl', 'ablaz', 'abnorm', 'abo', 'aboard', 'abod', 'abol', 'abolit', 'abomin', 'aborigin', 'abort', 'abound', 'about', 'abov', 'abr', 'abraham', 'abram', 'abras', 'abridg']\n"
     ]
    }
   ],
   "source": [
    "print(\"PorterStemmer:\")\n",
    "print(tfidf.get_feature_names()[:50])\n",
    "print()\n",
    "print(\"//-----------------//-----------------//-----------------//\")\n",
    "print()\n",
    "print(\"SnowballStemmer:\")\n",
    "print(tfidf2.get_feature_names()[:50])\n",
    "print()\n",
    "print(\"//-----------------//-----------------//-----------------//\")\n",
    "print()\n",
    "print(\"LancasterStemmer:\")\n",
    "print(tfidf3.get_feature_names()[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4308e54c",
   "metadata": {},
   "source": [
    "Logo na primeira entrada, podemos ver que tanto no PorterStemmer como no SnowballStemmer são iguais, mas no Lancaster já não. A primeira entrada no Lancaster, é a segunda entrada dos outros dois.\n",
    "\n",
    "Isto é um claro caso que o Lancaster descartu \"aaa\" como uma palavra importante para a análise do texto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121c63ef",
   "metadata": {},
   "source": [
    "<b>Conselho do Engenheiro: </b>\n",
    "\n",
    "Quando estamos a lidar com textos, como estes processos não são imediatos, devemos diferir os três stemmers (tfidf, tfidf2, tfidf3) e guardar estas representações num ficheiro pickle. Assim, já possuimos um modelo Bag of Words treinado e não é preciso passar pelo processo todo novamente.<br>\n",
    "Também se pode guardar os documentos após o processo stemming, dado que este também não é imediato."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298176c9",
   "metadata": {},
   "source": [
    "### Representação tf-idf após Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5fce89",
   "metadata": {},
   "source": [
    "Para esta secção estamos a usar o Docs2 que foi passado pelo LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3e79180c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 18959) <class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "X = tfidf3.transform(Docs2)\n",
    "tokens3 = tfidf3.get_feature_names()\n",
    "\n",
    "print(X.shape, type(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88744db",
   "metadata": {},
   "source": [
    "Temos 25000 documentos e 18959 dimensões, ou seja, palavras importantes.\n",
    "\n",
    "Vamos agora obter os valores, ordenados de forma decrescente, de cada uma destas palavras, em todos os documentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c62feea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palavras mais Importantes:\n",
      "['pokemon', 'dalm', 'libr', 'doodlebop', 'smallvil', 'scan', 'lupin', 'woo', 'sasquatch', 'wei', 'stev', 'darkm', 'colombo', 'zatoich', 'botch', 'demon', 'rap', 'muppet', 'bye', 'columbo', 'kibbutz', 'rajn', 'gadget', 'lennon', 'dari', 'ranm', 'naschy', 'melt', 'surf', 'seag', 'blah', 'dahm', 'grun', 'beetl', 'winfield', 'cyph', 'hackenstein', 'django', 'creasy', 'puppet', 'joan', 'othello', 'worm', 'gundam', 'noriko', 'suck', 'bird', 'camp', 'hanzo', 'zizek', 'dracul', 'hallam', 'scarecrow', 'zack', 'serb', 'gaur', 'genov', 'woody', 'gamer', 'pau', 'zomb', 'beatl', 'bruc', 'biko', 'keaton', 'alvin', 'cavem', 'leonor', 'oprah', 'khour', 'brend', 'pie', 'chib', 'elvir', 'solino', 'gandh', 'tan', 'rees', 'mordrid', 'gujarat', 'mindy', 'anny', 'karloff', 'montand', 'pia', 'lundgr', 'dent', 'menc', 'edy', 'gershon', 'indones', 'cognac', 'batm', 'modesty', 'kyl', 'tobacco', 'cordel', 'sarno', 'sandr', 'melind']\n",
      "\n",
      "//-----------------//-----------------//-----------------//\n",
      "\n",
      "Palavras menos Importantes:\n",
      "['nitro', 'ungrac', 'hickland', 'disabus', 'brigand', 'langford', 'suplex', 'christened', 'workload', 'abstin', 'toff', 'lautrec', 'dyspept', 'conto', 'mclaughlin', 'courth', 'authoress', 'inconsid', 'gucc', 'ebeneez', 'geometry', 'sylvain', 'gaspard', 'nobuhiro', 'dairy', 'sgc', 'sentry', 'statesm', 'sprinkles', 'airbrush', 'amorph', 'imposit', 'blameless', 'galadriel', 'bhatt', 'citadel', 'beslon', 'suw', 'ulliel', 'emerald', 'chessboard', 'streaked', 'reintegr', 'oncom', 'décor', 'surfeit', 'yodel', 'schubert', 'rasp', 'battleground', 'wooky', 'arw', 'chokeslam', 'dropkick', 'url', 'piglet', 'luncheon', 'nazare', 'traips', 'rateyourmus', 'ringsid', 'unplan', 'mothbal', 'schism', 'motorc', 'seydou', 'choisy', 'stealthy', 'gogh', 'hagiograph', 'striding', 'translit', 'gauch', 'manet', 'diaspor', 'foolproof', 'straightforwardly', 'underpaid', 'scullery', 'bookcas', 'tra', 'unread', 'oph', 'wein', 'faz', 'undercard', 'pasternak', 'concerto', 'monolith', 'fulgencio', 'unargu', 'zantar', 'telepathy', 'boardwalk', 'estell', 'kingsford', 'diagram', 'anathem', 'poncho', 'superplex']\n"
     ]
    }
   ],
   "source": [
    "xM = np.max(X, axis=0).toarray().squeeze()\n",
    "idx = np.argsort(-xM)\n",
    "voc3 = [tokens3[i] for i in idx]\n",
    "\n",
    "print(\"Palavras mais Importantes:\")\n",
    "print(voc3[:100])\n",
    "print()\n",
    "print(\"//-----------------//-----------------//-----------------//\")\n",
    "print()\n",
    "print(\"Palavras menos Importantes:\")\n",
    "print(voc3[-100:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d7965f",
   "metadata": {},
   "source": [
    "Algumas destas palavras estão cortadas pelo stem e podem não fazer muito sentido. Podemos comparar ao que eram, caso apareçam, se não tivesse sido feito o stem.\n",
    "\n",
    "Para este caso, no TfidfVectorizer, o parametero \"min_df\" é 5 e o \"token_pattern\" aceita palavras com 4 caracteres no mínimo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a2a66a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palavras mais Importantes:\n",
      "['pokemon', 'scanners', 'steve', 'doodlebops', 'demons', 'casper', 'sasquatch', 'smallville', 'darkman', 'xica', 'weller', 'sucks', 'zatoichi', 'lennon', 'botched', 'gadget', 'worms', 'cypher', 'naschy', 'ants', 'janeane', 'blah', 'priya', 'muppet', 'joan', 'tanner', 'lexi', 'hackenstein', 'othello', 'seagal', 'gamera', 'zizek', 'alvin', 'brendan', 'khouri', 'pack', 'beatles', 'ranma', 'chiba', 'darius', 'wrestlemania', 'flea', 'karloff', 'dentist', 'gruner', 'paulie', 'hanzo', 'oprah', 'elvira', 'montand', 'bruce', 'zombi', 'keaton', 'camp', 'winfield', 'dahmer', 'coop', 'django', 'stefan', 'columbo', 'sissy', 'beetle', 'blob', 'sandra', 'batman', 'azumi', 'britney', 'gershon', 'hulk', 'macarthur', 'reese', 'kusturica', 'kornbluth', 'ichi', 'annie', 'zack', 'pusser', 'gundam', 'mordrid', 'dracula', 'mary', 'mindy', 'dillinger', 'jill', 'librarians', 'danish', 'scarecrows', 'biko', 'nerd', 'program', 'lundgren', 'edie', 'chen', 'shearer', 'moto', 'sniper', 'scooby', 'nacho', 'gerry', 'julian']\n",
      "\n",
      "//-----------------//-----------------//-----------------//\n",
      "\n",
      "Palavras menos Importantes:\n",
      "['generously', 'characteristically', 'unrepentant', 'lenin', 'eyewitness', 'outcry', 'utilised', 'purveyor', 'labourer', 'skinning', 'trots', 'quip', 'disclose', 'sagas', 'conjures', 'bourbon', 'confiscated', 'perched', 'unsavoury', 'unwisely', 'lifeboats', 'nursed', 'dubs', 'confessing', 'tanovic', 'peculiarly', 'empowered', 'confides', 'assassinates', 'jurisdiction', 'unlocked', 'unspectacular', 'squash', 'plunder', 'bibles', 'enlisting', 'cineplex', 'negotiate', 'radiating', 'sharpshooter', 'britons', 'annihilated', 'deportation', 'extravaganzas', 'collier', 'tellingly', 'père', 'levene', 'usefulness', 'amplify', 'leverage', 'decayed', 'melchior', 'patrolman', 'swells', 'basking', 'denim', 'mistreatment', 'ancestral', 'reverting', 'temperamental', 'confidante', 'immunity', 'subtracted', 'inciting', 'administered', 'ralli', 'distort', 'roughing', 'slyly', 'southampton', 'nyree', 'vertical', 'alloy', 'orientated', 'spacious', 'pressuring', 'backfire', 'attained', 'manically', 'booed', 'mclaughlin', 'livelier', 'gliding', 'sylvain', 'auspicious', 'inconsiderate', 'ware', 'galadriel', 'coaxing', 'oncoming', 'uphold', 'oversee', 'cataclysmic', 'hypocrites', 'thieving', 'emerald', 'songwriting', 'gauche', 'suplexes']\n"
     ]
    }
   ],
   "source": [
    "Docs = D.data\n",
    "Docs = [doc.decode('UTF-8') for doc in Docs]\n",
    "Docs = [doc.replace('<br />', ' ') for doc in Docs]\n",
    "Docs = [re.sub(r'[^a-zA-Z\\u00C0-\\u00FF]+', ' ', doc) for doc in Docs]\n",
    "\n",
    "tfidf = TfidfVectorizer(min_df = 5, token_pattern = r'\\b\\w\\w\\w\\w+\\b').fit(Docs)\n",
    "tokens = tfidf.get_feature_names()\n",
    "\n",
    "X = tfidf.transform(Docs)\n",
    "xM = np.max(X, axis=0).toarray().squeeze()\n",
    "idx = np.argsort(-xM)\n",
    "voc = [tokens[i] for i in idx]\n",
    "\n",
    "print(\"Palavras mais Importantes:\")\n",
    "print(voc[:100])\n",
    "print()\n",
    "print(\"//-----------------//-----------------//-----------------//\")\n",
    "print()\n",
    "print(\"Palavras menos Importantes:\")\n",
    "print(voc[-100:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c469ff",
   "metadata": {},
   "source": [
    "Também podemos obter as palavras que aparecem mais vezes ao longo de todos os documentos, a partir do idf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7cf2683e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'that', 'with', 'movie', 'have', 'film', 'from', 'like', 'they', 'there', 'just', 'about', 'what', 'some', 'good', 'more', 'when', 'time', 'very', 'even', 'only', 'would', 'really', 'story', 'which', 'well', 'than', 'much', 'their', 'were', 'other', 'been', 'most', 'also', 'into', 'first', 'made', 'great', 'because', 'will', 'people', 'make', 'could', 'after', 'then', 'them', 'watch', 'think', 'acting', 'movies', 'seen', 'characters', 'many', 'plot', 'being', 'never', 'best', 'little', 'character', 'ever', 'know', 'where', 'over', 'better', 'life', 'films', 'does', 'love', 'still', 'here', 'your', 'these', 'while', 'should', 'something', 'such', 'through', 'back', 'scenes', 'watching', 'scene', 'those', 'thing', 'real', 'years', 'doesn', 'actors', 'another', 'before', 'though', 'director', 'makes', 'work', 'didn', 'look', 'actually', 'nothing', 'find', 'going', 'show']\n"
     ]
    }
   ],
   "source": [
    "idf = tfidf.idf_\n",
    "idx = np.argsort(idf)\n",
    "voc = [tokens[i] for i in idx]\n",
    "print(voc[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ed09e1",
   "metadata": {},
   "source": [
    "Como podemos observar, estas palavras são neutras a criticas positivas ou negativas, pois são palavras para construir frases \"normais\". Não existe nenhuma palavra que seja importante para a distinção entre os ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02250e70",
   "metadata": {},
   "source": [
    "## Análise dos Coeficientes dum Discriminante Logístico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f4d0eec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0d9d51a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = load_files('PreparacaoLaboratorioTexto/aclImdb/train/')\n",
    "Docs1 = D.data\n",
    "y1 = D.target\n",
    "\n",
    "D = load_files('PreparacaoLaboratorioTexto/aclImdb/test/')\n",
    "Docs2 = D.data\n",
    "y2 = D.target\n",
    "\n",
    "Docs1 = [doc.decode('UTF-8') for doc in Docs1]\n",
    "Docs1 = [doc.replace('<br />', ' ') for doc in Docs1]\n",
    "Docs1 = [re.sub(r'[^a-zA-Z\\u00C0-\\u00FF]+', ' ', doc) for doc in Docs1]\n",
    "\n",
    "tfidf = TfidfVectorizer(min_df = 5, token_pattern = r'\\b\\w\\w\\w\\w+\\b').fit(Docs1)\n",
    "X1 = tfidf.transform(Docs)\n",
    "\n",
    "Docs2 = [doc.decode('UTF-8') for doc in Docs2]\n",
    "Docs2 = [doc.replace('<br />', ' ') for doc in Docs2]\n",
    "Docs2 = [re.sub(r'[^a-zA-Z\\u00C0-\\u00FF]+', ' ', doc) for doc in Docs2]\n",
    "\n",
    "X2 = tfidf.transform(Docs2)\n",
    "\n",
    "xM = np.max(X, axis=0).toarray().squeeze()\n",
    "idx = np.argsort(-xM)\n",
    "voc = [tokens[i] for i in idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8625c3",
   "metadata": {},
   "source": [
    "A razão para não fazermos stem neste caso, é apenas para podermos ter uma melhor ideia das palavras são classificadas como positivas e negativas, sem estarem reduzidas.\n",
    "\n",
    "Vamos proceder ao processo de classificação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e3187fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.98516\n",
      "0.87088\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "dl = LogisticRegression(penalty = 'l2', max_iter = 1000, C = 10, tol = 1e-3)\n",
    "dl.fit(X1, y1)\n",
    "\n",
    "print(dl.score(X1, y1))\n",
    "print(dl.score(X2, y2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7954b6cc",
   "metadata": {},
   "source": [
    "Como podemos observar, apesar dos resultados não serem maus (cerca de 87% de acertos no teste), existe uma descrepância entre os valores de acertos no treino e no teste.\n",
    "\n",
    "Vamos aumentar o factor de regularização, C. <b>Não esquecer que este valor é inversamente proporcional ao lambda das fórmulas do Ridge e do Lasso</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d8da9fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.93248\n",
      "0.87968\n"
     ]
    }
   ],
   "source": [
    "dl = LogisticRegression(penalty = 'l2', max_iter = 1000, C = 1, tol = 1e-3)\n",
    "dl.fit(X1, y1)\n",
    "\n",
    "print(dl.score(X1, y1))\n",
    "print(dl.score(X2, y2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4643ff9",
   "metadata": {},
   "source": [
    "Assim reduzimos a diferença de resultados entre os valores de treino e do teste.\n",
    "\n",
    "Vamos observar os coeficientes, w."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "126c89af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pesos com menor valor:\n",
      "['aames', 'aamir', 'aankhen', 'aardman', 'aaron', 'aback', 'abandon', 'abandoned', 'abandoning', 'abandonment', 'abandons', 'abbas', 'abbey', 'abbot', 'abbott', 'abbreviated', 'abby', 'abduct', 'abducted', 'abduction', 'abel', 'abetted', 'abhay', 'abhishek', 'abhorrent', 'abiding', 'abigail', 'abilities', 'ability', 'abject', 'able', 'ably', 'abnormal', 'aboard', 'abode', 'abominable', 'abomination', 'abominations', 'aboriginal', 'aborigine', 'aborigines', 'aborted', 'abortion', 'abortions', 'abound', 'abounds', 'about', 'above', 'abraham', 'abrahams']\n",
      "\n",
      "//-----------------//-----------------//-----------------//\n",
      "\n",
      "Pesos com maior valor:\n",
      "['zeenat', 'zeffirelli', 'zeitgeist', 'zelah', 'zelda', 'zemeckis', 'zenda', 'zenith', 'zentropa', 'zero', 'zest', 'zesty', 'zeta', 'zeus', 'zhang', 'zhivago', 'zhou', 'ziegfeld', 'ziering', 'ziggy', 'zilch', 'zillion', 'zimmer', 'zing', 'zion', 'zippy', 'ziyi', 'zizek', 'zodiac', 'zombi', 'zombie', 'zombies', 'zombified', 'zomcom', 'zone', 'zoned', 'zones', 'zoolander', 'zoom', 'zooming', 'zooms', 'zoot', 'zorro', 'zucco', 'zucker', 'zuckerman', 'zulu', 'zuniga', 'zwick', 'émigré']\n"
     ]
    }
   ],
   "source": [
    "w = dl.coef_\n",
    "idx = np.argsort(w).squeeze()\n",
    "\n",
    "voc = tfidf.get_feature_names()\n",
    "voc2 = [voc[i] for i in idx]\n",
    "\n",
    "print(\"Pesos com menor valor:\")\n",
    "print(voc[:50])\n",
    "print()\n",
    "print(\"//-----------------//-----------------//-----------------//\")\n",
    "print()\n",
    "print(\"Pesos com maior valor:\")\n",
    "print(voc[-50:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1383f145",
   "metadata": {},
   "source": [
    "Concluimos que os primeiros 50 valores têm conotação negativa.<br>\n",
    "Os últimos 50 valores têm conotação positiva.\n",
    "\n",
    "Para ver quantas dimensões existem após a operação de representação do TfidfVectorizer, basta sabermos o shape do w."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d9b561ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 25871)\n"
     ]
    }
   ],
   "source": [
    "print(w.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833c832a",
   "metadata": {},
   "source": [
    "Como vimos várias vezes anteriormente, existem mais de 25000 dimensões, ou seja, palavras que têm significado na classificação das críticas.\n",
    "\n",
    "Como as passamos por um discriminante com regularização L2, Ridge, algumas destas foram convertidas para valores muito próximos de 0, ou seja, tiverem muito pouca influência na classificação das criticas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9b4b8480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "w = w.squeeze()\n",
    "\n",
    "plt.plot(w)\n",
    "\n",
    "print(np.sum(np.abs(w) <= 1e-3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c757426f",
   "metadata": {},
   "source": [
    "Apenas 120 pesos têm um valor muito próximo de 0.<br>\n",
    "Pelo plot, podemos ver que existe alguns pesos que se destacam tanto para valores muito altos, como um perto de 7.5, outro acima de 5. Mas também temos os valores muito baixos como um próximo de -10, e outro perto de -7.5.\n",
    "\n",
    "Vamos treinar o nosso determinante logístico, mas desta vez com a regularização L1, Lasso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "506a64f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.89848\n",
      "0.87284\n"
     ]
    }
   ],
   "source": [
    "dl = LogisticRegression(penalty = 'l1', solver = 'saga', max_iter = 1000, C = 1, tol = 1e-3)\n",
    "dl.fit(X1, y1)\n",
    "\n",
    "print(dl.score(X1, y1))\n",
    "print(dl.score(X2, y2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719614b2",
   "metadata": {},
   "source": [
    "Comparando ao Ridge, temos um score ligeiramente mais reduzido, assim como a descrepância entre eles.\n",
    "\n",
    "No Lasso, as dimensões não são convertidas para valores próximos de 0, mas sim de 0. Vamos ver quantas dimensões foram tidas em conta para este discrimante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2f48575c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1096\n"
     ]
    }
   ],
   "source": [
    "w = dl.coef_\n",
    "w = w.squeeze()\n",
    "\n",
    "print(np.sum(w != 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0c8814",
   "metadata": {},
   "source": [
    "Surpreendentemente, apenas 1097 dimensões foram tidas em conta para treinar este modelo.\n",
    "\n",
    "Vamos observar as palavras que discriminam entre uma critica negativa e uma positiva."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7edaba1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pesos com menor valor:\n",
      "['worst', 'waste', 'awful', 'poorly', 'disappointment', 'boring', 'fails', 'dull', 'worse', 'pointless', 'horrible', 'mess', 'poor', 'disappointing', 'lacks', 'terrible', 'unfunny', 'avoid', 'laughable', 'annoying', 'badly', 'unfortunately', 'ridiculous', 'forgettable', 'nothing', 'save', 'redeeming', 'lame', 'wooden', 'supposed', 'unless', 'instead', 'lousy', 'script', 'mediocre', 'mildly', 'pathetic', 'weak', 'dreadful', 'wonder', 'basically', 'insult', 'baldwin', 'stupid', 'obnoxious', 'tedious', 'incoherent', 'uninteresting', 'predictable', 'minutes']\n",
      "\n",
      "//-----------------//-----------------//-----------------//\n",
      "\n",
      "Pesos com maior valor:\n",
      "['atmosphere', 'captures', 'surprised', 'heart', 'powerful', 'fascinating', 'world', 'also', 'delightful', 'simple', 'enjoy', 'outstanding', 'still', 'masterpiece', 'unique', 'hilarious', 'love', 'entertaining', 'liked', 'surprisingly', 'moving', 'well', 'beautiful', 'touching', 'funniest', 'finest', 'beautifully', 'incredible', 'enjoyed', 'perfectly', 'definitely', 'noir', 'subtle', 'enjoyable', 'highly', 'loved', 'fantastic', 'today', 'brilliant', 'superb', 'favorite', 'rare', 'refreshing', 'best', 'amazing', 'wonderfully', 'wonderful', 'great', 'perfect', 'excellent']\n"
     ]
    }
   ],
   "source": [
    "voc = tfidf.get_feature_names()\n",
    "idx = np.argsort(w)\n",
    "voc2 = [voc[i] for i in idx]\n",
    "\n",
    "print(\"Pesos com menor valor:\")\n",
    "print(voc2[:50])\n",
    "print()\n",
    "print(\"//-----------------//-----------------//-----------------//\")\n",
    "print()\n",
    "print(\"Pesos com maior valor:\")\n",
    "print(voc2[-50:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61d8f4b",
   "metadata": {},
   "source": [
    "Se aumentássemos o valor de C, o que concluiriamos seria que a descrepância no resultado entre treino e o teste seria muito maior, e o resultado do treino também seria melhor. Mas como visto previamente, isto não significa que o nosso modelo é melhor.\n",
    "\n",
    "Por outro lado, se diminuíssemos este valor, a descrepância seria menor, assim como ambos os resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e59532",
   "metadata": {},
   "source": [
    "### N-gramas\n",
    "\n",
    "N-gramas são dimensões que em vem de possuirem uma única palavra, possuem n palavras.<br>\n",
    "Isto pode ser vantajoso embora seja dispendioso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ff013e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "962a086a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137344\n"
     ]
    }
   ],
   "source": [
    "D = load_files('PreparacaoLaboratorioTexto/aclImdb/train/')\n",
    "Docs1 = D.data\n",
    "y1 = D.target\n",
    "\n",
    "D = load_files('PreparacaoLaboratorioTexto/aclImdb/test/')\n",
    "Docs2 = D.data\n",
    "y2 = D.target\n",
    "\n",
    "Docs1 = [doc.decode('UTF-8') for doc in Docs1]\n",
    "Docs1 = [doc.replace('<br />', ' ') for doc in Docs1]\n",
    "Docs1 = [re.sub(r'[^a-zA-Z\\u00C0-\\u00FF]+', ' ', doc) for doc in Docs1]\n",
    "\n",
    "tfidf = TfidfVectorizer(min_df = 5, ngram_range = (1, 2), token_pattern = r'\\b\\w\\w\\w+\\b').fit(Docs1)\n",
    "X1 = tfidf.transform(Docs)\n",
    "\n",
    "Docs2 = [doc.decode('UTF-8') for doc in Docs2]\n",
    "Docs2 = [doc.replace('<br />', ' ') for doc in Docs2]\n",
    "Docs2 = [re.sub(r'[^a-zA-Z\\u00C0-\\u00FF]+', ' ', doc) for doc in Docs2]\n",
    "\n",
    "X2 = tfidf.transform(Docs2)\n",
    "\n",
    "print(X1.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0435cdf9",
   "metadata": {},
   "source": [
    "Com n-gramas de uma palavra e duas palavras, o nosso dicionário aumenta de forma exponencial, como podemos observar.<br>\n",
    "Todo o processo foi muito mais demorado.\n",
    "\n",
    "Novamente, a razão de não existir stemming, é para conseguir perceber melhor que palavras são tidas em contas no modelo.\n",
    "\n",
    "Vamos observar as palavras mais importantes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4b821e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['scanners', 'pokemon', 'wei', 'dev', 'casper', 'more more', 'woo', 'botched', 'xica', 'hackenstein', 'steve', 'zizek', 'darkman', 'khouri', 'demons', 'zatoichi', 'gadget', 'lennon', 'smallville', 'tanner', 'othello', 'shearer', 'naschy', 'sasquatch', 'darius', 'coop', 'beetle', 'gamera', 'weller', 'stefan', 'seagal', 'joan', 'cypher', 'ants', 'soha', 'alvin', 'librarians', 'dahmer', 'chiba', 'paulie', 'kyle', 'kriemhild', 'janeane', 'biko', 'winfield', 'che', 'jill', 'basket', 'worms', 'muppet', 'puppet master', 'macarthur', 'foo', 'pack', 'timon', 'kusturica', 'brendan', 'creasy', 'ranma', 'django', 'kornbluth', 'dillinger', 'kolchak', 'dominick', 'hanzo', 'zack', 'elvira', 'blank check', 'lincoln', 'gruner', 'zombi', 'the doodlebops', 'karloff', 'ripley', 'oprah', 'raj', 'bruce', 'azumi', 'wrestlemania', 'lassie', 'pierce', 'homer', 'doodlebops', 'ichi', 'sucks', 'tobacco', 'luzhin', 'modesty', 'lexi', 'jigsaw', 'morty', 'mindy', 'europa', 'moto', 'dressler', 'gypo', 'mathieu', 'lundgren', 'nacho', 'old person']\n"
     ]
    }
   ],
   "source": [
    "xM = np.max(X1, axis = 0).toarray().squeeze()\n",
    "idx = np.argsort(-xM)\n",
    "tokens = tfidf.get_feature_names()\n",
    "voc = [tokens[i] for i in idx]\n",
    "print(voc[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d869891c",
   "metadata": {},
   "source": [
    "Também podemos saber quantos uni-gramas existem, assim como bi-gramas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ad84042b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de Uni-gramas:  26718\n",
      "Número de Bi-gramas:  110626\n"
     ]
    }
   ],
   "source": [
    "wC = np.array([len(w.split()) for w in voc])\n",
    "\n",
    "print(\"Número de Uni-gramas: \", np.sum(wC == 1))\n",
    "print(\"Número de Bi-gramas: \", np.sum(wC == 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9863c8d5",
   "metadata": {},
   "source": [
    "Como podemos observar, o número de bi-gramas no nosso vocabulário é muito maior que o número de uni-gramas.<br>\n",
    "Se aumentássemos o número máximo na gama de n-gramas, este número seria cada vez maior.\n",
    "\n",
    "Vamos agora classificar as críticas com o nosso discriminante logístico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8121d00a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.77824\n",
      "0.78084\n",
      "56\n"
     ]
    }
   ],
   "source": [
    "dl = LogisticRegression(penalty = 'l1', solver = 'saga', max_iter = 1000, C = 0.10, tol = 1e-3)\n",
    "dl.fit(X1, y1)\n",
    "\n",
    "print(dl.score(X1, y1))\n",
    "print(dl.score(X2, y2))\n",
    "\n",
    "w = dl.coef_\n",
    "w = w.squeeze()\n",
    "\n",
    "print(np.sum(w != 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a62c560",
   "metadata": {},
   "source": [
    "Neste caso, a nossa regularização é muito forte, C = 0.10.<br>\n",
    "O número de pesos que tiveram influência foram 57 das iniciais 137344. Isto é uma redução muito severa, especialmente quando anteriormente conseguimos taxas de acerto a rondar os 90% e agora temos a rondar os 78%.\n",
    "\n",
    "Vamos olhar para os n-gramas que tiveram mais influência para classificar as criticas, tanto para as negativas como para as positivas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612eb088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pesos com menor valor:\n",
      "['worst', 'bad', 'waste', 'awful', 'boring', 'nothing', 'poor', 'terrible', 'even', 'just', 'minutes', 'plot', 'stupid', 'script', 'there', 'they', 'why', 'any', 'worse', 'only', 'was', 'horrible', 'this', 'would', 'make', 'movie', 'money', 'supposed', 'not', 'have', 'could', 'off', 'instead', 'thing', 'then', 'have been', 'remembered for', 'remembered one', 'remembered and', 'aaa', 'remember which', 'remember you', 'remember with', 'remember why', 'remember who', 'remembered seeing', 'remember where', 'remember when', 'remember what', 'remember well']\n",
      "\n",
      "//-----------------//-----------------//-----------------//\n",
      "\n",
      "Pesos com maior valor:\n",
      "['games but', 'gandhi the', 'games are', 'game you', 'gandolfini', 'gangs new', 'gangs and', 'gangs', 'game when', 'gangly', 'gangland', 'gang who', 'gang thugs', 'gang the', 'gandhi was', 'gang that', 'game where', 'gang rape', 'gang members', 'gang leader', 'game which', 'gang are', 'gang and', 'game with', 'gang all', 'gang', 'game would', 'gandolfini and', 'gang raped', 'games and', 'beautiful', 'fun', 'still', 'his', 'favorite', 'loved', 'will', 'life', 'also', 'the best', 'amazing', 'perfect', 'very', 'well', 'and', 'love', 'wonderful', 'best', 'excellent', 'great']\n"
     ]
    }
   ],
   "source": [
    "voc = tfidf.get_feature_names()\n",
    "idx = np.argsort(w)\n",
    "voc2 = [voc[i] for i in idx]\n",
    "\n",
    "print(\"Pesos com menor valor:\")\n",
    "print(voc2[:50])\n",
    "print()\n",
    "print(\"//-----------------//-----------------//-----------------//\")\n",
    "print()\n",
    "print(\"Pesos com maior valor:\")\n",
    "print(voc2[-50:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1625e757",
   "metadata": {},
   "source": [
    "Vamos agora alterar o valor da regularização e repetir este processo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c027fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8882\n",
      "0.87972\n",
      "717\n",
      "Pesos com menor valor:\n",
      "['worst', 'waste', 'awful', 'poorly', 'boring', 'bad', 'dull', 'poor', 'disappointment', 'fails', 'horrible', 'worse', 'annoying', 'mess', 'disappointing', 'terrible', 'unfortunately', 'not worth', 'pointless', 'lacks', 'laughable', 'nothing', 'avoid', 'ridiculous', 'badly', 'save', 'lame', 'supposed', 'unfunny', 'script', 'stupid', 'redeeming', 'instead', 'pathetic', 'forgettable', 'minutes', 'unless', 'crap', 'than this', 'not even', 'way too', 'predictable', 'basically', 'wooden', 'wasted', 'wonder', 'looks', 'attempt', 'not very', 'weak']\n",
      "\n",
      "//-----------------//-----------------//-----------------//\n",
      "\n",
      "Pesos com maior valor:\n",
      "['especially', 'moving', 'seen', 'hilarious', 'world', 'job', 'surprised', 'enjoy', 'also', 'entertaining', 'both', 'still', 'better than', 'beautifully', 'well', 'love', 'gem', 'touching', 'the best', 'incredible', 'bit', 'liked', 'simple', 'this great', 'funniest', 'beautiful', 'perfectly', 'noir', 'definitely', 'enjoyed', 'best', 'loved', 'fun', 'refreshing', 'rare', 'highly', 'fantastic', 'enjoyable', 'must see', 'well worth', 'brilliant', 'superb', 'today', 'wonderfully', 'favorite', 'amazing', 'wonderful', 'great', 'perfect', 'excellent']\n"
     ]
    }
   ],
   "source": [
    "dl = LogisticRegression(penalty = 'l1', solver = 'saga', max_iter = 1000, C = 1, tol = 1e-3)\n",
    "dl.fit(X1, y1)\n",
    "\n",
    "print(dl.score(X1, y1))\n",
    "print(dl.score(X2, y2))\n",
    "\n",
    "w = dl.coef_\n",
    "w = w.squeeze()\n",
    "\n",
    "print(np.sum(w != 0))\n",
    "\n",
    "voc = tfidf.get_feature_names()\n",
    "idx = np.argsort(w)\n",
    "voc2 = [voc[i] for i in idx]\n",
    "\n",
    "print(\"Pesos com menor valor:\")\n",
    "print(voc2[:50])\n",
    "print()\n",
    "print(\"//-----------------//-----------------//-----------------//\")\n",
    "print()\n",
    "print(\"Pesos com maior valor:\")\n",
    "print(voc2[-50:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285caf93",
   "metadata": {},
   "source": [
    "Com C = 1 conseguimos um melhor resultado, não só em termos da reduzida descrepância como em termos de resultados. 88%.<br>\n",
    "Também observamos que as dimensões tidas em conta foram 713.\n",
    "\n",
    "Vamos tentar com o C = 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af48f111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.99756\n",
      "0.88508\n",
      "6547\n",
      "Pesos com menor valor:\n",
      "['waste', 'not worth', 'worst', 'awful', 'disappointment', 'poorly', 'forgettable', 'laughable', 'lacks', 'boring', 'mess', 'the introduction', 'worse', 'unfunny', 'horrible', 'fails', 'lousy', 'avoid', 'blue and', 'not recommend', 'poor', 'alright', 'only good', 'disappointing', 'baldwin', 'annoying', 'badly', 'obnoxious', 'great loved', 'unlikeable', 'pointless', 'save', 'let down', 'basically', 'bad', 'very disappointed', 'had high', 'dull', 'not recommended', 'fast forward', 'might enjoy', 'the excellent', 'unwatchable', 'unconvincing', 'pretentious', 'unfortunately', 'wooden', 'ridiculous', 'wanted like', 'skip this']\n",
      "\n",
      "//-----------------//-----------------//-----------------//\n",
      "\n",
      "Pesos com maior valor:\n",
      "['finest', 'can wait', 'gem', 'chilling', 'better than', 'predictable but', 'recommended', 'may not', 'underrated', 'endearingly', 'atmosphere', 'love this', 'perfectly', 'highly recommend', 'better but', 'subtle', 'today', 'likable and', 'brilliant', 'must see', 'british comedy', 'perfect the', 'his way', 'favorite', 'you only', 'kitty', 'captures', 'enjoyable', 'loved this', 'amazing', 'wonderful', 'not disappointed', 'whoopi', 'great', 'superb', 'funniest', 'incredible', 'cerebral', 'lot things', 'flawless', 'surprisingly good', 'rare', 'wonderfully', 'refreshing', 'evil breed', 'this great', 'perfect', 'well worth', 'excellent', 'definitely worth']\n"
     ]
    }
   ],
   "source": [
    "dl = LogisticRegression(penalty = 'l1', solver = 'saga', max_iter = 1000, C = 10, tol = 1e-3)\n",
    "dl.fit(X1, y1)\n",
    "\n",
    "print(dl.score(X1, y1))\n",
    "print(dl.score(X2, y2))\n",
    "\n",
    "w = dl.coef_\n",
    "w = w.squeeze()\n",
    "\n",
    "print(np.sum(w != 0))\n",
    "\n",
    "voc = tfidf.get_feature_names()\n",
    "idx = np.argsort(w)\n",
    "voc2 = [voc[i] for i in idx]\n",
    "\n",
    "print(\"Pesos com menor valor:\")\n",
    "print(voc2[:50])\n",
    "print()\n",
    "print(\"//-----------------//-----------------//-----------------//\")\n",
    "print()\n",
    "print(\"Pesos com maior valor:\")\n",
    "print(voc2[-50:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecc9d87",
   "metadata": {},
   "source": [
    "Para este caso, a descrepância é muito elevada, cerca de 11%. O resultado dos dados de treino é muito perto de 100%, isto é um indicador de sobreaprendizagem.<br>\n",
    "Também podemos observar que a regularização foi menos severa e estrita que anteriormente. Temos 6792 dimensões a serem tidas em conta."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
